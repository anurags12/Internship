{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d31faa-0e89-4602-a448-74bfe5fdc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abcce6-0365-414b-89d9-bb258091773b",
   "metadata": {},
   "source": [
    "1 -- Write a python program to display IMDB’s Top rated 100 Indian movies’ data\r\n",
    "https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58f31b4a-a07a-40c5-ab1a-89d7934576d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbd_movies_top_100(url):\n",
    "    name=[]\n",
    "    rating=[]\n",
    "    year=[]\n",
    "    page=requests.get(url)\n",
    "    if page.status_code==200:\n",
    "        soup=BeautifulSoup(page.content)\n",
    "        for n in soup.find_all('h3',class_=\"lister-item-header\"):\n",
    "            name.append(re.sub('[^A-Za-z ]+',' ',n.text))\n",
    "        for r in soup.find_all('div',class_=\"ipl-rating-star small\"):\n",
    "            rating.append(r.text.replace('\\n',''))\n",
    "        for y in soup.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "            year.append(re.sub('[()]','',y.text))\n",
    "        df=pd.DataFrame({'Name':name,'Rating':rating,'Year_of_release':year})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eafcb6-62ba-430b-92e0-c6232dd929d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a563df8a-dd6a-4bf5-b68b-63dfe8575834",
   "metadata": {},
   "source": [
    "5 -- Write a python program to scrape house details from mentioned URL. It should include house title, location,\r\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\r\n",
    "Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2efe0bd-d344-4001-b59e-d01de51c06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def house_details(url):\n",
    "    title=[]\n",
    "    location=[]\n",
    "    area=[]\n",
    "    emi=[]\n",
    "    price=[]\n",
    "    page=requests.get(url)\n",
    "    if page.status_code==200:\n",
    "        soup=BeautifulSoup(page.content)\n",
    "        for t in soup.find_all('a',class_=\"overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full\"):\n",
    "            title.append(t.text)\n",
    "        for l in soup.find_all('div',class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0.1p po:max-w-95\"):\n",
    "            location.append(re.sub('Independent House,?\\s+|\\xa0','',l.text))\n",
    "        for aep in soup.find_all('div',class_=\"font-semi-bold heading-6\"):\n",
    "            for a in re.finditer('\\d+.*\\s+sqft',aep.text):\n",
    "                area.append(a.group().replace('sqft',''))\n",
    "            for e in re.finditer('₹.*/Month',aep.text):\n",
    "                emi.append(e.group().replace('₹',''))\n",
    "            for p in re.finditer('₹.*Crores?|₹\\d{2}\\s+Lacs',aep.text):\n",
    "                price.append(p.group().replace('₹',''))\n",
    "        df=pd.DataFrame({'House_title':title,'Location':location,'Area(sqft)':area,'EMI':emi,'Price':price})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b4d79-8644-48a4-9a14-72917f6caf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a433163-d6c7-478a-91b3-c33cfa10b5d4",
   "metadata": {},
   "source": [
    "6 -- Write a python program to scrape first 10 product details which include product name , price , Image URL from\r\n",
    "https://www.bewakoof.com/bestseller?sort=popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67033ccc-1121-4709-8ed9-e1aa9f15cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bewakoof_first_10(url):\n",
    "    name=[]\n",
    "    price=[]\n",
    "    URL=[]\n",
    "    page=requests.get(url)\n",
    "    if page.status_code==200:\n",
    "        soup=BeautifulSoup(page.content)\n",
    "        for n in soup.find_all('div',class_=\"productNaming bkf-ellipsis\"):\n",
    "            name.append(n.text)\n",
    "        for p in soup.find_all('a',class_=\"col-sm-4 col-xs-6 px-2\"):\n",
    "            price.append(p.text.split('₹')[1])\n",
    "        for i in soup.find_all('img',class_=\"productImgTag\"):\n",
    "            URL.append(i['src'])\n",
    "        df=pd.DataFrame({'Product_name':name,'Price':price,'Image_URL':URL})\n",
    "        return df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e77814-42a1-45f7-976c-f48900be5280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55b2bbad-05fb-4ecb-8b0d-a178b7cdc51f",
   "metadata": {},
   "source": [
    "7 -- Please visit https://www.cnbc.com/world/?region=world and scrap\u0002a) headings\r\n",
    "b) date\r\n",
    "c) News link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bc0e02b-8162-4f5b-a733-0d7d3e2a6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnbc_news(url):\n",
    "    heading=[]\n",
    "    date=[]\n",
    "    link=[]\n",
    "    page=requests.get(url)\n",
    "    if page.status_code==200:\n",
    "        soup=BeautifulSoup(page.content)\n",
    "        for t in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "            heading.append(t.text)\n",
    "            date.append(re.search('(\\d{4})/(\\d{2})/(\\d{2})',str(t)).group())\n",
    "            link.append(re.search('https?:/+.+\\.html?',str(t)).group())\n",
    "        df=pd.DataFrame({'News_heading':heading,'Date':date,'News_link':link})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98220ceb-0cbe-4766-9a25-9bb16ad643d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97ee2a99-6c4a-4edf-b5e5-bc9440ce4861",
   "metadata": {},
   "source": [
    "8--Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded\u0002articles/ and scrap-\r\n",
    " a) Paper title\r\n",
    " b) date\r\n",
    " c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "954893ff-f223-496b-8c51-496c0df7c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def journal_authors(url):\n",
    "    title=[]\n",
    "    date=[]\n",
    "    author=[]\n",
    "    page=requests.get(url)\n",
    "    if page.status_code==200:\n",
    "        soup=BeautifulSoup(page.content)\n",
    "        for t in soup.find_all('h2',class_=\"h5 article-title\"):\n",
    "            title.append(re.sub('\\n|\\r\\s+','',t.text))\n",
    "        for d in soup.find_all('p',class_=\"article-date\"):\n",
    "            date.append(d.text)\n",
    "        for a in soup.find_all('p',class_=\"article-authors\"):\n",
    "            author.append(a.text.replace('  ',''))\n",
    "        df=pd.DataFrame({'Paper_Title':title,'Date':date,'Author_name':author})\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
